{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'networkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6bfe7eca7b88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnetworkx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinkprediction\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnxadapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'networkit'"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from networkit import linkprediction as lp, nxadapter\n",
    "from functools import partial \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_label(pair, graph):\n",
    "    u, v = pair[0], pair[1]\n",
    "    return (int(graph.hasEdge(u, v)))\n",
    "\n",
    "\n",
    "def concatenate(node_set, label):\n",
    "    dataset = pd.DataFrame({'nodes': node_set, 'label': label})\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Create training and testing graphs, compute feature engineering\n",
    "    and save datasets.\n",
    "    \"\"\"\n",
    "\n",
    "    # Graph import\n",
    "    G = nx.read_edgelist('data/graph.txt', comments='#')\n",
    "    valid_graph = nxadapter.nx2nk(G)\n",
    "\n",
    "    # Training and test graphs creation\n",
    "    test_graph = lp.RandomLinkSampler.byPercentage(valid_graph, 0.9)\n",
    "    train_graph = lp.RandomLinkSampler.byPercentage(test_graph, 0.7)\n",
    "\n",
    "    # Training and testing sets creation\n",
    "    testing_set = lp.MissingLinksFinder(test_graph).findAtDistance(2)\n",
    "    training_set = lp.MissingLinksFinder(train_graph).findAtDistance(2)\n",
    "\n",
    "    # Label creation\n",
    "    y_train = list(map(partial(assign_label, graph=test_graph), training_set))\n",
    "    y_test = list(map(partial(assign_label, graph=valid_graph), testing_set))\n",
    "\n",
    "    # Concatenation of labels with samples\n",
    "    train = concatenate(training_set, y_train)\n",
    "    test = concatenate(testing_set, y_test)\n",
    "    trainingSet = train.nodes.values\n",
    "    testingSet = test.nodes.values\n",
    "\n",
    "    # Feature engineering\n",
    "    trainLPs = [\n",
    "        lp.CommonNeighborsIndex(train_graph), lp.JaccardIndex(train_graph),\n",
    "        lp.AdamicAdarIndex(train_graph), lp.ResourceAllocationIndex(train_graph),\n",
    "        lp.PreferentialAttachmentIndex(train_graph), lp.AdjustedRandIndex(train_graph),\n",
    "        lp.NeighborhoodDistanceIndex(train_graph), lp.TotalNeighborsIndex(train_graph),\n",
    "        lp.SameCommunityIndex(train_graph), lp.UDegreeIndex(train_graph),\n",
    "        lp.VDegreeIndex(train_graph)\n",
    "    ]\n",
    "\n",
    "    testLPs = [\n",
    "        lp.CommonNeighborsIndex(test_graph), lp.JaccardIndex(test_graph),\n",
    "        lp.AdamicAdarIndex(test_graph), lp.ResourceAllocationIndex(test_graph),\n",
    "        lp.PreferentialAttachmentIndex(test_graph), lp.AdjustedRandIndex(test_graph),\n",
    "        lp.NeighborhoodDistanceIndex(test_graph), lp.TotalNeighborsIndex(test_graph),\n",
    "        lp.SameCommunityIndex(test_graph), lp.UDegreeIndex(test_graph), lp.VDegreeIndex(test_graph)\n",
    "    ]\n",
    "\n",
    "    X_train = lp.getFeatures(trainingSet, *trainLPs)\n",
    "    X_test = lp.getFeatures(testingSet, *testLPs)\n",
    "\n",
    "    # Concatenate features with samples and labels\n",
    "    features = ['CN', 'JC', 'AA', 'RA', 'PA', 'AR', 'ND', 'TN', 'SC', 'UD', 'VD']\n",
    "    train_features = pd.DataFrame(X_train, columns=features)\n",
    "    test_features = pd.DataFrame(X_test, columns=features)\n",
    "    train = pd.concat([train, train_features], axis=1)\n",
    "    test = pd.concat([test, test_features], axis=1)\n",
    "\n",
    "    # Export files as csv\n",
    "    train.to_csv('data/train.csv', sep=';', header=True, decimal='.', encoding='utf-8', index=False)\n",
    "    test.to_csv('data/test.csv', sep=';', header=True, decimal='.', encoding='utf-8', index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"main\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
